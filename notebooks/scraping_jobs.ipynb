{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee05995-a2f1-4ae0-9086-2593aa7b2309",
   "metadata": {},
   "source": [
    "# Scraping websites for job recommendation algorithm\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "##### Input/Output :\n",
    "- Input ðŸ“¥: \n",
    "    - ```data/raw/geoId.csv```\n",
    "- Output ðŸ“¤:\n",
    "    - ```data/processed/geoId.csv```\n",
    "\n",
    "\n",
    "##### Information about the notebook ðŸ“˜:\n",
    "- This notebooks aims to **find best job offers for you by web scraping**. As a reminder, **web scraping is the process of gathering information from the Internet, most of the time automatically**. Just to make sure you understand the the scope of this process, scraping a page respectfully for educational purposes is not a problem since the information is publically available.\n",
    "\n",
    "\n",
    "- I choosen to use **BeautifulSoup** librairy because it's an easy one for beginners (for other librairies, see Selenium, lxml, Scrapy..). BeautifulSoup is a Python library for parsing structured data (```soup = BeautifulSoup(page.content, \"html.parser\"```). It allows you to interact with HTML in a similar way to how you interact with a web page using developer tools. Indeed, an HTML web page is structured by **tags** making elements search simple: \n",
    "    - find elements by class name: ```element1 = soup.find_all(\"<tag>\", class_=\"<class>\")```\n",
    "    - find elements by id: ```element2 = soup.find_all(\"<tag>\", id_=\"<id>\")```\n",
    "    - find elements by text content: ```element3 = soup.find_all(\"<tag>\", string=\"<string>\")```\n",
    " \n",
    "\n",
    "\n",
    "- In this notebook, scraping and parsing data process enables to gather information about job offers: 'Title', 'Company', 'Country', 'City', 'Salary', 'Summary, 'Date', 'Job_id' and 'Job_url'. The job recommendation algorithm can process **several websites**, **countries**, **cities** and **pages**. For *LinkedIn* website, the parameter geoId was required to scrap data on multiple cities. I retrieved information about geoId from *https://help4access.com/no-more-secrets/* into ```data/raw/geoId.csv```, then I cleaned and saved data in ```data/processed/geoId.csv```.\n",
    "\n",
    "- The **jobs recommendation algorithm** takes in argument a dictionary with information about the user request: **jobs_parameters**.\n",
    "``` bash\n",
    "jobs_parameters = {\n",
    "    'website': ['indeed', 'linkedin'],\n",
    "    'query': 'Data Scientist',\n",
    "    'location': ['Geneva', 'Paris', 'Brussels', 'Amsterdam'],\n",
    "    'distance': 10,\n",
    "    'description_keywords_ordered': ['Data Science', 'Deep Learning', 'Machine Learning', 'AWS', 'Data', 'Python', 'SQL','Analysis', 'Modelling'],\n",
    "    'description_keywords_excluded': ['Headhunter', 'Manager', 'Director', 'Senior'],\n",
    "    'title_keywords_must': ['Data'],\n",
    "    'title_keywords_ordered': ['Junior', 'Data Scientist', 'Internship', 'Data Science', 'DataScientist', 'DataScience'],\n",
    "    'title_keywords_excluded': ['Manager', 'Director', 'Senior', 'Head', 'Freelance', 'Engineer', 'Experienced'],\n",
    "    'pages': 20\n",
    "}\n",
    "```      \n",
    "\n",
    "\n",
    "- It is possible that **LinkedIn blocked you while scraping the website**. You'll get the error mentioned below. Indeed you can only access a LinkedIn profile if you are logged in and when LinkedIn receives a request, it looks for a specific cookie called **li_at** in the request. If it does not find this cookie, it redirects the requester to a page with the JavaScript you had. This JavaScript serves to redirect you to the login page. That's what all the ```window.location.href=<thing>``` is about. You juste have to add the li_at cookie value: ``` request = requests.get('https://www.linkedin.com/in/<your_profile>/', headers={'cookie': 'li_at=<cookie_li_at_value>'})```. \\\n",
    "You can \"**fake**\" a logged-in request by going to LinkedIn, copying your own li_at cookie, and adding that to your request. Note that this will only work temporarily: at some point LinkedIn will expect that cookie to change, and you will have to re-copy it.\n",
    "``` bash\n",
    "<html><head>\n",
    "<script type=\"text/javascript\">\n",
    "window.onload = function() {\n",
    "  // Parse the tracking code from cookies.\n",
    "  var trk = \"bf\";\n",
    "  var trkInfo = \"bf\";\n",
    "  var cookies = document.cookie.split(\"; \");\n",
    "  for (var i = 0; i < cookies.length; ++i) {\n",
    "    if ((cookies[i].indexOf(\"trkCode=\") == 0) && (cookies[i].length > 8)) {\n",
    "      trk = cookies[i].substring(8);\n",
    "    }\n",
    "    else if ((cookies[i].indexOf(\"trkInfo=\") == 0) && (cookies[i].length > 8)) {\n",
    "      trkInfo = cookies[i].substring(8);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if (window.location.protocol == \"http:\") {\n",
    "    // If \"sl\" cookie is set, redirect to https.\n",
    "    for (var i = 0; i < cookies.length; ++i) {\n",
    "      if ((cookies[i].indexOf(\"sl=\") == 0) && (cookies[i].length > 3)) {\n",
    "        window.location.href = \"https:\" + window.location.href.substring(window.location.protocol.length);\n",
    "        return;\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Get the new domain. For international domains such as\n",
    "  // fr.linkedin.com, we convert it to www.linkedin.com\n",
    "  var domain = \"www.linkedin.com\";\n",
    "  if (domain != location.host) {\n",
    "    var subdomainIndex = location.host.indexOf(\".linkedin\");\n",
    "    if (subdomainIndex != -1) {\n",
    "      domain = \"www\" + location.host.substring(subdomainIndex);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  window.location.href = \"https://\" + domain + \"/authwall?trk=\" + trk + \"&trkInfo=\" + trkInfo +\n",
    "      \"&originalReferer=\" + document.referrer.substr(0, 200) +\n",
    "      \"&sessionRedirect=\" + encodeURIComponent(window.location.href);\n",
    "}\n",
    "</script>\n",
    "</head></html>\n",
    "``` \n",
    "\n",
    "<!-- <img src=\"\" style=\"width: 2000px; height: 500\">\n",
    "\n",
    "<img src=\"\" style=\"width: 2000px; height: 600\"> -->\n",
    "\n",
    "\n",
    "##### Summary ðŸ“‚:\n",
    "\n",
    " - Part 1 - Imports\n",
    " \n",
    " - Part 2 - Clean geoId data (required for LinkedIn)\n",
    " \n",
    " - Part 3 - Jobs recommendation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "###                                     ###\n",
    "###  Author: Bilal Fatian               ###\n",
    "###  Date: 20/09/2023                   ###\n",
    "###                                     ###\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6c9db-91b0-4fd8-9d46-734ea0f29065",
   "metadata": {},
   "source": [
    "## Part 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0494ef9-22ce-411e-8da4-371a274c9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    " \n",
    "import json, csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import iso3166\n",
    "from geopy.geocoders import Nominatim\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import webbrowser\n",
    "import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509aeb07-e4b2-4d40-b0ef-5d8b9b37b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the full text of a pandas DataFrame (with none of its values truncated).\n",
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321011d-e5d5-4510-80c6-61adaeae7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LI_AT_COOKIE = \"ENTER YOUR LI AT COOKIE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901ae9e-0bf2-4b46-98bf-e8b29cfbd348",
   "metadata": {},
   "source": [
    "## Part 2 - Clean geoId data (required for LinkedIn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5906a-d19a-48fa-a86e-6d21211bb335",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe67a5-6188-464e-84e9-230bf8a0d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(csv_file):\n",
    "    \"\"\" Read data from csv file\n",
    "    Args:\n",
    "        csv_file: String, csv filename\n",
    "    Returns:\n",
    "        df: Dataframe, contains data from csv file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except:\n",
    "        df = pd.read_csv(csv_file, delimiter=';')\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_address(address, address_type):\n",
    "    \"\"\" Process data address to country, region and city\n",
    "    Args:\n",
    "        address: String, whole address (city, region, country)\n",
    "        address_type: String, address type to process\n",
    "    Returns:\n",
    "        addr: String, processed address (either country, region or city)\n",
    "    \"\"\"\n",
    "    addr_tab = address.split(\",\")\n",
    "    if len(addr_tab) == 3:\n",
    "        \n",
    "        if address_type == \"CITY\":\n",
    "            addr = addr_tab[0]\n",
    "        elif address_type == \"REGION\":\n",
    "            addr = addr_tab[1]\n",
    "        elif address_type == \"COUNTRY\":\n",
    "            addr = addr_tab[2]\n",
    "            \n",
    "    elif len(addr_tab) == 2:\n",
    "        if address_type == \"CITY\" or address_type == \"REGION\":\n",
    "            addr = addr_tab[0]\n",
    "        elif address_type == \"COUNTRY\":\n",
    "            addr = addr_tab[1]\n",
    "    else:\n",
    "        addr = \"None\"\n",
    "    return addr   \n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\" Clean data into dataframe\n",
    "    Args:\n",
    "        df: Dataframe to clean\n",
    "    Returns:\n",
    "        df: Dataframe, cleaned dataframe\n",
    "    \"\"\"\n",
    "    # Create 3 rows with location information\n",
    "    df['CITY'] = df['ADDRESS'].apply(transform_address, address_type='CITY')\n",
    "    df['REGION'] = df['ADDRESS'].apply(transform_address, address_type='REGION')\n",
    "    df['COUNTRY'] = df['ADDRESS'].apply(transform_address, address_type='COUNTRY')\n",
    "    \n",
    "    # Select columns to process and sort df by country code\n",
    "    df = df[['COUNTRY_CODE', 'COUNTRY', 'REGION', 'CITY', 'GEO_ID']].sort_values(by='COUNTRY_CODE')\n",
    "    \n",
    "    # Remove duplicates, unknowned country code and country\n",
    "    df.dropna(subset = ['COUNTRY_CODE'], inplace=True)\n",
    "    df = df.drop(df[df['COUNTRY'] == 'None'].index)\n",
    "    df = df.drop_duplicates().reset_index()\n",
    "    df = df.drop(columns=['index'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data_geoId(geoId_csv = \"../data/raw/geoId.csv\"):\n",
    "    \"\"\" Clean geoId data for jobs recommendation algorithm\n",
    "    Args:\n",
    "        geoId_csv: String, csv filename to clean\n",
    "    Returns:\n",
    "        df_geoId: Dataframe, contains geoId data\n",
    "    \"\"\"\n",
    "    # Read geoId data\n",
    "    df = read_data(geoId_csv)\n",
    "    \n",
    "    # Clean geoId data\n",
    "    df_geoId = clean_data(df)\n",
    "    \n",
    "    # Save processed df into csv file\n",
    "    geoId_csv_processed = \"../data/processed/geoId.csv\"\n",
    "    try:\n",
    "        df_geoId.to_csv(geoId_csv_processed)\n",
    "        print(\"CSV file '{}' has been cleaned and saved into '{}'\".format(geoId_csv, geoId_csv_processed))\n",
    "    except:\n",
    "        print(\"Error while saving CSV file  into '{}'\".format(geoId_csv_processed))\n",
    "        \n",
    "    return df_geoId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f464c-2954-4a8b-bf30-8a53d5292694",
   "metadata": {},
   "source": [
    "##### Clean and create processed geoId csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ccae9-ec35-4204-a8da-1df0810c9d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoId_csv = \"../data/raw/geoId.csv\"\n",
    "geoId_csv_processed = geoId_csv.replace('raw', 'processed')\n",
    "\n",
    "if not os.path.isfile(geoId_csv_processed):\n",
    "    df_geoId = clean_data_geoId(geoId_csv)\n",
    "    print(\">> File '{}' created\".format(geoId_csv_processed))\n",
    "else:\n",
    "    print(\">> File '{}' already exists\".format(geoId_csv_processed))\n",
    "    df_geoId = read_data(geoId_csv_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919c125-a63d-4db3-9260-53e1cd0185ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geoId.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d5040-2136-46d3-80c2-ed08f7e0173e",
   "metadata": {},
   "source": [
    "##### Add geoId data to geoId csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7944a-fedb-435e-b08d-c8c1b2a42cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geoId(df_geoId, dic_geoId):\n",
    "    \"\"\" Add geoId data\n",
    "    Args:\n",
    "        df_geoId: Dataframe, contains geoId data\n",
    "        dic_geoId: Dictionary with geoId information {COUNTRY_CODE:country_code , COUNTRY:country, REGION:region, CITY:city, GEO_ID:geo_id}\n",
    "    Returns:\n",
    "        df_geoId: Dataframe, contains geoId data with information addition\n",
    "    \"\"\"\n",
    "    country_code = dic_geoId['COUNTRY_CODE']\n",
    "    country = dic_geoId['COUNTRY']\n",
    "    region = dic_geoId['REGION']\n",
    "    city = dic_geoId['CITY']\n",
    "    geo_id = dic_geoId['GEO_ID']\n",
    "    \n",
    "    \n",
    "    if geo_id in list(df_geoId['GEO_ID'].values):\n",
    "        print(\">> Geo_id '{}' already in dataframe df_geoId\".format(geo_id))\n",
    "        \n",
    "    # Add new geo_id\n",
    "    else:\n",
    "        new_df = pd.DataFrame(data=dic_geoId, index=[0])\n",
    "        df_geoId = pd.concat([df_geoId, new_df]).sort_values(by='COUNTRY_CODE').reset_index()\n",
    "        df_geoId = df_geoId.drop(columns=['index'])\n",
    "        print(\">> dic_geoId '{}' successfully added in dataframe df_geoId\\n\".format(dic_geoId))\n",
    "        \n",
    "        # Save processed df into csv file\n",
    "        geoId_csv_processed = \"../data/processed/geoId.csv\"\n",
    "        try:\n",
    "            df_geoId.to_csv(geoId_csv_processed)\n",
    "            print(\">> CSV file '{}' saved into '{}'\\n\".format(geoId_csv, geoId_csv_processed))\n",
    "        except:\n",
    "            print(\">> Error while saving CSV file  into '{}'\\n\".format(geoId_csv_processed))\n",
    "        \n",
    "    return df_geoId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df9cc9-04c4-4c51-a889-0d2dd3c792d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_geoId = {'COUNTRY_CODE':'FR', 'COUNTRY':'France', 'REGION':'Ile-de-France', 'CITY':'Massy', 'GEO_ID':'106185287'}\n",
    "# df_geoId = add_geoId(df_geoId, dic_geoId)\n",
    "# df_geoId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc20da-64b1-47a6-a636-8d03b66f6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic_geoId_list = [{'COUNTRY_CODE':'FR', 'COUNTRY':'France', 'REGION':'Ile-de-France', 'CITY':'Epinay-sur-Orge', 'GEO_ID':'103354451'},\n",
    "#              {'COUNTRY_CODE':'FR', 'COUNTRY':'France', 'REGION':'Ile-de-France', 'CITY':'Savigny-sur-Orge', 'GEO_ID':'106556981'},\n",
    "#              {'COUNTRY_CODE':'FR', 'COUNTRY':'France', 'REGION':'Ile-de-France', 'CITY':'Arpajon', 'GEO_ID':'106057890'},\n",
    "#              {'COUNTRY_CODE':'FR', 'COUNTRY':'France', 'REGION':'Ile-de-France', 'CITY':'Longjumeau', 'GEO_ID':'104220344'}\n",
    "#             ]\n",
    "# for dic_geoId in dic_geoId_list:\n",
    "#     add_geoId(df_geoId, dic_geoId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca76f3-aebc-48c7-95e5-25e29ead59f3",
   "metadata": {},
   "source": [
    "## Part 3 - Jobs recommendation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da2342-3332-4372-900e-deae696ce479",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fcd4f-c2c4-4310-8c81-87e6b368f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df2csv(df_jobs, filename_csv):\n",
    "    \"\"\" Save dataframe into csv file\n",
    "    Args:\n",
    "        df_jobs: Dataframe, contains information about scrapped jobs\n",
    "        filename_csv: String, filename csv\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_jobs.to_csv(filename_csv, sep=';')\n",
    "        print(\">> File '{}' successfully saved\".format(filename_csv))\n",
    "    except:\n",
    "        print(\">> Error while saving file '{}'\".format(filename_csv))\n",
    "\n",
    "        \n",
    "def convert_csv2json(csv_filename, json_filename):\n",
    "    \"\"\" Convert csv file to json file\n",
    "    Args:\n",
    "        csv_filename: String, csv filename\n",
    "        json_filename: String, json filename\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    json_tab = []\n",
    "    with open(csv_filename, encoding='utf-8') as csv_f: \n",
    "        # Load csv file data using csv library's dictionary reader\n",
    "        csvReader = csv.DictReader(csv_f, delimiter=';') \n",
    "\n",
    "        # Convert each csv row into python dict\n",
    "        for row in csvReader: \n",
    "            #add this python dict to json array\n",
    "            json_tab.append(row)\n",
    "  \n",
    "    # Convert python json_tab to Json string and write to file\n",
    "    with open(json_filename, 'w', encoding='utf-8') as json_f: \n",
    "        json_str = json.dumps(json_tab, indent=4, separators=(', ', ': '))\n",
    "        json_f.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7845792-5639-42e5-9203-c6d058fabf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_in_dic_recursively(search_dict, field):\n",
    "    \"\"\"\n",
    "    Takes a dict with nested lists and dicts, and searches all dicts for a key of the field provided.\n",
    "    Args:\n",
    "        search_dict: Dictionary\n",
    "        field: String, field to find\n",
    "    Returns:\n",
    "        fields_found: Array of strings, contains fiels found\n",
    "    \"\"\"\n",
    "    fields_found = []\n",
    "\n",
    "    for key, value in search_dict.items():\n",
    "\n",
    "        if key == field:\n",
    "            fields_found.append(value)\n",
    "\n",
    "        elif isinstance(value, dict):\n",
    "            results = get_field_in_dic_recursively(value, field)\n",
    "            for result in results:\n",
    "                fields_found.append(result)\n",
    "\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    more_results = get_field_in_dic_recursively(item, field)\n",
    "                    for another_result in more_results:\n",
    "                        fields_found.append(another_result)\n",
    "\n",
    "    return fields_found\n",
    "\n",
    "def remove_elements_end_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Remove elements at the end of sentence\n",
    "    Args:\n",
    "        sentence: String, sentence\n",
    "    Returns:\n",
    "        sentence: String, processed sentence\n",
    "    \"\"\"\n",
    "    excluded_elements = [\".\", \",\", \";\", \" \"]\n",
    "    if isinstance(sentence, str):\n",
    "        while (len(sentence)>1) and (sentence[-1] in excluded_elements):\n",
    "            sentence = sentence[:-2]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d32ed-1b9d-46b0-ad8d-f7ad3383101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_title(website, item, jobs_parameters):\n",
    "    \"\"\" Scrap title\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "        jobs_parameters: Dictionay, contains information about user request\n",
    "    Returns:\n",
    "        job_title: String, job title\n",
    "        job_title_rating: Integer, job title rating\n",
    "    \"\"\"\n",
    "    job_title = \"\"\n",
    "    title_keywords_must = [word.lower() for word in jobs_parameters['title_keywords_must']]\n",
    "    title_keywords_excluded = [word.lower() for word in jobs_parameters['title_keywords_excluded']]\n",
    "    \n",
    "    if website == 'Indeed':\n",
    "        job_title = item.find_all('h2', class_=\"jobTitle\")[0].find_all('span')[-1]\n",
    "\n",
    "    elif website == 'LinkedIn':\n",
    "        job_title = item.find_all('h3', class_=\"base-search-card__title\")[0]\n",
    "\n",
    "    # Check valid title and rate title\n",
    "    job_title = job_title.text.strip()\n",
    "    title_tmp = job_title.lower()\n",
    "\n",
    "    # Remove title without must have keywords\n",
    "    if len(title_keywords_must)>0 and title_keywords_must[0] != '':\n",
    "        for title_must in title_keywords_must:\n",
    "            if title_must not in title_tmp:\n",
    "                job_title = \"\"\n",
    "                break\n",
    "\n",
    "    # Remove excluded title keywords\n",
    "    if len(title_keywords_excluded)>0 and title_keywords_excluded[0] != '':\n",
    "        for title_excluded in title_keywords_excluded:\n",
    "            if title_excluded in title_tmp:\n",
    "                job_title = \"\"\n",
    "                break\n",
    "\n",
    "    return job_title\n",
    "\n",
    "\n",
    "def get_job_company_name(website, item):\n",
    "    \"\"\" Scrap job company name\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "        job_company_name: String, job company name\n",
    "    \"\"\"\n",
    "    job_company_name = \"\"\n",
    "\n",
    "    if website == 'Indeed':\n",
    "        job_company_name = item.find_all('span', class_=\"companyName\")[0]\n",
    "    elif website == 'LinkedIn':\n",
    "        job_company_name = item.find_all('h4', class_=\"base-search-card__subtitle\")[0]\n",
    "            \n",
    "    job_company_name = job_company_name.text.strip().upper()\n",
    "    return job_company_name\n",
    "\n",
    "\n",
    "def get_job_company_location(website, item):\n",
    "    \"\"\" Scrap job company location\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "        job_company_location: String job company location\n",
    "    \"\"\"\n",
    "    job_company_location = \"\"\n",
    " \n",
    "    if website == 'Indeed':\n",
    "        job_company_location = item.find_all('div', class_=\"companyLocation\")[0]\n",
    "    elif website == 'LinkedIn':\n",
    "        job_company_location = item.find_all('span', class_=\"job-search-card__location\")[0]\n",
    "\n",
    "    job_company_location = job_company_location.text.strip().split(',')[0]\n",
    "    return job_company_location\n",
    "\n",
    "\n",
    "def get_job_rating(website, item):\n",
    "    \"\"\" Scrap job rating\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "        job_rating: String job rating\n",
    "    \"\"\"\n",
    "    job_rating = \"\"\n",
    "    if website == 'Indeed':\n",
    "        try:\n",
    "            job_rating = item.find_all('span', class_=\"ratingNumber\")[0].text\n",
    "        except:\n",
    "            job_rating = \"\"\n",
    "            \n",
    "    return job_rating\n",
    "\n",
    "\n",
    "def get_job_salary(website, item):\n",
    "    \"\"\" Scrap job salary\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    job_salary = \"\"\n",
    "    \n",
    "    if website == 'Indeed':\n",
    "        try:\n",
    "            job_salary = item.find_all('div', class_=\"metadata salary-snippet-container\")[0]\n",
    "            job_salary = job_salary.text.strip()\n",
    "        except:\n",
    "            job_salary = \"\"\n",
    "            \n",
    "    return job_salary\n",
    "    \n",
    "    \n",
    "def get_job_summary(website, item):\n",
    "    \"\"\" Scrap job summary\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "        job_summary: String, job summary\n",
    "    \"\"\"\n",
    "    job_summary = \"\"\n",
    "    \n",
    "    if website == 'Indeed':\n",
    "        job_summary = item.find_all('div', class_=\"job-snippet\")[0]\n",
    "        job_summary = job_summary.text.strip().replace('\\n', ' ')\n",
    "        \n",
    "    elif website == 'LinkedIn':\n",
    "        pass\n",
    "        \n",
    "    return job_summary\n",
    "\n",
    "    \n",
    "def get_job_date(website, item):\n",
    "    \"\"\" Scrap job date\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "        job_date: String, job date\n",
    "    \"\"\"\n",
    "    job_date = \"\"\n",
    "    \n",
    "    if website == 'Indeed':\n",
    "        job_date = item.find_all('span', class_='date')[0]\n",
    "        \n",
    "    elif website == 'LinkedIn':\n",
    "        job_date = item.find_all('time')[0]\n",
    "    \n",
    "    job_date = job_date.text.strip()\n",
    "    digits = [digit for digit in job_date if digit.isdigit()]\n",
    "    digits = ''.join(digits)\n",
    "    \n",
    "    if website == 'LinkedIn':\n",
    "        if 'minute' in job_date:\n",
    "            digits = \"<1\"\n",
    "        elif 'hour' in job_date:\n",
    "            digits = \"<1\"\n",
    "        elif 'week' in job_date:\n",
    "            digits = 7*int(digits)\n",
    "        elif 'month' in job_date:\n",
    "            digits = 30*int(digits)\n",
    "        else:\n",
    "            digits = int(digits)\n",
    "       \n",
    "    job_date = \"{} day ago\".format(digits)\n",
    "\n",
    "    if not job_date[1].isdigit():\n",
    "        job_date = \"0{}\".format(job_date)\n",
    "    return job_date\n",
    "\n",
    "\n",
    "def get_job_id(website, item):\n",
    "    \"\"\" Scrap job id\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "        job_id: String, job id\n",
    "    \"\"\"\n",
    "    if website == 'Indeed':\n",
    "        job_id = item['data-jk']\n",
    "    elif website == 'LinkedIn':\n",
    "        job_id = item['data-entity-urn'].split(':')[-1]\n",
    "    return job_id\n",
    "\n",
    "\n",
    "def get_job_url(website, item, url, job_id):\n",
    "    \"\"\" Scrap job url\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        item: Array of strings, contains HTML elements\n",
    "    Returns:\n",
    "        job_url: String, job url\n",
    "    \"\"\"\n",
    "    if website == 'Indeed':\n",
    "        try:\n",
    "            data_empn = item['data-empn']\n",
    "            job_url = \"{}&advn={}&vjk={}\".format(url, data_empn, job_id)\n",
    "        except:\n",
    "            job_url = \"{}&vjk={}\".format(url, job_id)\n",
    "    elif website == 'LinkedIn':\n",
    "        try:\n",
    "            job_url = item.find_all('a', class_='base-card__full-link')[0]['href']\n",
    "        except:\n",
    "            job_url = \"\"\n",
    "    return job_url\n",
    "\n",
    "def set_company_type(nb_employees):\n",
    "    \"\"\" Scrap job company location\n",
    "    Args:\n",
    "        nb_employees: String, employees number in the company\n",
    "    Returns:\n",
    "        job_company_type: String job company type\n",
    "    \"\"\"\n",
    "    # Set min and max employees\n",
    "    try:\n",
    "        start, end = nb_employees.split('-')\n",
    "        start, end = int(start), int(end)\n",
    "    except:\n",
    "        start = int(nb_employees)\n",
    "        end = None\n",
    "\n",
    "    if start >= 5000: # Large Enterprise \n",
    "        job_company_type = 'Large Enterprise (+5000 employees)'\n",
    "    elif end is not None:\n",
    "        if end < 5000: # Intermediate-sized Enterprise\n",
    "            job_company_type = 'Intermediate-sized Enterprise (251-5000 employees)'\n",
    "        if end <= 250: # Medium-sized Enterprise\n",
    "            job_company_type = 'Medium-sized Enterprise (51-250 employees)'\n",
    "        if end <= 50 : # Small-sized Enterprise\n",
    "            job_company_type = 'Small-sized Enterprise (11-50 employees)'   \n",
    "        if end <= 10: # Startup\n",
    "            job_company_type = 'Startup (1-10 employees)'\n",
    "    else:\n",
    "        job_company_type = \"Unknown\"\n",
    "    \n",
    "    return job_company_type\n",
    "\n",
    "\n",
    "def get_job_company_type(website, job_company_name, recurs=2):\n",
    "    \"\"\" Scrap job company location\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        job_company_name: String, company name\n",
    "    Returns:\n",
    "        job_company_type: String job company type\n",
    "    \"\"\"\n",
    "    job_company_type = \"\"\n",
    "    job_company_name = job_company_name.lower().replace(' ','-').replace('&','and')\n",
    "    url = \"https://www.linkedin.com/company/{}/about/\".format(job_company_name)\n",
    "    \n",
    "    try:        \n",
    "        # Make request with Beautiful Soup (headers='<headers={'cookie': 'li_at=<cookie_li_at_value>'})```>' as explained in the summary)\n",
    "        headers = {'cookie': 'li_at={}'.format(LI_AT_COOKIE)}\n",
    "        soup = request_bs4(url, headers=headers)\n",
    "\n",
    "        # Parse data\n",
    "        item_tab = soup.find_all('code')\n",
    "        nb_employees = 0\n",
    "        i = 0\n",
    "        while i<len(item_tab) and nb_employees == 0 :\n",
    "            item = item_tab[i].text.strip()\n",
    "            \n",
    "            # Convert object into dictionary\n",
    "            item_dic = json.loads(item)\n",
    "            \n",
    "            # Find the key 'staffCountRange' recursively in the dictionary\n",
    "            if isinstance(item_dic, dict):\n",
    "                staff_tab = get_field_in_dic_recursively(item_dic, 'staffCountRange')\n",
    "                for staff_dic in staff_tab:\n",
    "                    \n",
    "                     # Find the key 'start' recursively in the dictionary\n",
    "                    if isinstance(staff_dic, dict):\n",
    "                        start = get_field_in_dic_recursively(staff_dic, 'start')\n",
    "                        try:\n",
    "                            nb_employees = staff_dic['start']\n",
    "\n",
    "                            # Try to extract maximum company size ('end' variable)\n",
    "                            try:\n",
    "                                end = get_field_in_dic_recursively(staff_dic, 'end')[0]\n",
    "                                nb_employees = \"{}-{}\".format(nb_employees, end)\n",
    "                            except:\n",
    "                                break\n",
    "                        except:\n",
    "                            pass\n",
    "            i+=1\n",
    "        # Select company size type \n",
    "        job_company_type = set_company_type(nb_employees)\n",
    "        \n",
    "    except:\n",
    "        job_company_type = \"Unknown\"\n",
    "        if recurs > 0:\n",
    "            job_company_name = job_company_name.rsplit('-', 1)[0] # remove last word\n",
    "            while job_company_name[-1] == '-':\n",
    "                job_company_name = job_company_name[:-1]\n",
    "            recurs -= 1\n",
    "            job_company_type = get_job_company_type(website, job_company_name, recurs=recurs)\n",
    "        \n",
    "    return job_company_type\n",
    "\n",
    "\n",
    "def get_job_company_sector(website, job_company_name, recurs=2):\n",
    "    \"\"\" Scrap job company location\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        job_company_name: String, company name\n",
    "    Returns:\n",
    "        job_company_sector: String, job company sector\n",
    "    \"\"\"\n",
    "    job_company_sector = \"\"\n",
    "    job_company_name = job_company_name.lower().replace(' ','-').replace('&','and')\n",
    "    url = \"https://www.linkedin.com/company/{}/about/\".format(job_company_name)\n",
    "\n",
    "    try:        \n",
    "        # Make request with Beautiful Soup (headers='<headers={'cookie': 'li_at=<cookie_li_at_value>'})```>' as explained in the summary)\n",
    "        headers = {'cookie': 'li_at={}'.format(LI_AT_COOKIE)}\n",
    "        soup = request_bs4(url, headers=headers)\n",
    "\n",
    "        # Parse data\n",
    "        item_tab = soup.find_all('code')\n",
    "        i = 0\n",
    "        job_company_sector = \"\"\n",
    "        while i<len(item_tab) and job_company_sector == \"\":\n",
    "            item = item_tab[i].text.strip()\n",
    "\n",
    "            # Convert object into dictionary\n",
    "            item_dic = json.loads(item)\n",
    "\n",
    "            # Find the key 'companyType' recursively in the dictionary\n",
    "            if isinstance(item_dic, dict):\n",
    "                sector_tab = get_field_in_dic_recursively(item_dic, 'specialities')\n",
    "                for sector in sector_tab:\n",
    "                    if isinstance(sector, list):\n",
    "                        job_company_sector = ', '.join(sector)\n",
    "            i+=1\n",
    "        \n",
    "    except:\n",
    "        job_company_sector = \"Unknown\"\n",
    "        if recurs > 0:\n",
    "            job_company_name = job_company_name.rsplit('-', 1)[0] # remove last word\n",
    "            while job_company_name[-1] == '-':\n",
    "                job_company_name = job_company_name[:-1]\n",
    "            recurs -= 1\n",
    "            job_company_sector = get_job_company_sector(website, job_company_name, recurs=recurs)\n",
    "        \n",
    "    return job_company_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081818c-170c-41d7-babb-8c69b4634b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_countries_dic(city_tab):\n",
    "    \"\"\" Create dictionary with countries and cities\n",
    "    Args:\n",
    "        city_tab: Array of strings, cities name\n",
    "    Returns:\n",
    "        country_dic: Dictionary, contains cities in country ({COUNTRY_A:[CITY_A, CITY_B], COUNTRY_B:CITY_C})\n",
    "    \"\"\"\n",
    "    # Tool to search OSM (Open Street Map) data by name and address (geocoding) \n",
    "    geolocator = Nominatim(user_agent=\"http\")\n",
    "    geocode = partial(geolocator.geocode, language=\"en\")\n",
    "    \n",
    "    # Fill country/cities dictionary\n",
    "    country_dic = {}\n",
    "    city_tab = set(city_tab)\n",
    "    for city_to_add in city_tab:\n",
    "        \n",
    "        # Find country by selected city\n",
    "        country_to_add = geocode(city_to_add)\n",
    "        country_to_add = str(country_to_add).upper().split(',')[-1]\n",
    "        \n",
    "        if country_to_add[0] == ' ':\n",
    "            country_to_add = country_to_add[1:]\n",
    "        \n",
    "        if len(country_dic) == 0:\n",
    "            country_dic.update({country_to_add:city_to_add})\n",
    "        else:\n",
    "            for countries, cities in country_dic.items():\n",
    "                # if country already exists\n",
    "                if countries == country_to_add:\n",
    "                    \n",
    "                    # If there is one city\n",
    "                    if type(cities) is str:\n",
    "                        city_to_add = [cities, city_to_add]\n",
    "                    else:\n",
    "                        city_to_add = cities + [city_to_add]   \n",
    "                        \n",
    "                    country_dic[country_to_add] = city_to_add\n",
    "                    break\n",
    "\n",
    "            country_dic.update({country_to_add:city_to_add})\n",
    "\n",
    "    return country_dic\n",
    "\n",
    "\n",
    "def get_country_code(country):\n",
    "    \"\"\" Get country code from country\n",
    "    Args:\n",
    "        country: String, country name\n",
    "    Returns:\n",
    "        country_code: Integer, country code\n",
    "    \"\"\"\n",
    "    # List of country codes\n",
    "    country_code_list = iso3166.countries_by_name\n",
    "    \n",
    "    # Country code search\n",
    "    for countries, information in country_code_list.items():\n",
    "        if country in countries:\n",
    "            country_code = country_code_list[countries].alpha2.lower()\n",
    "            break\n",
    "    return country_code\n",
    "\n",
    "\n",
    "def find_geoId(city, geoId_csv = \"../data/processed/geoId.csv\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        city: String, name of the city to find geoId\n",
    "        geoId_csv: String, csv filename where geoIds are stored\n",
    "    Returns:\n",
    "        geoId: Integer, geoId of the city\n",
    "    \"\"\"\n",
    "    df = read_data(geoId_csv)\n",
    "    geoId = df[df['CITY'] == city]['GEO_ID'].values[0]\n",
    "    return geoId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c90d6d-29bd-4bab-b99a-1755c6ead069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_indeed(country, city, page, jobs_parameters):\n",
    "    \"\"\" Create url for indeed scrapping\n",
    "    Args:\n",
    "        country: String, country name\n",
    "        city: String, city name\n",
    "        page: Integer, page numero\n",
    "        jobs_parameters: Dictionay, contains information about user request\n",
    "    Returns:\n",
    "        url: String, url\n",
    "    \"\"\"\n",
    "    country_code = get_country_code(country) \n",
    "    query = jobs_parameters['query'].replace(' ', '%20')\n",
    "    distance = jobs_parameters['distance']\n",
    "    page = str(page*10)\n",
    "    url = \"https://{}.indeed.com/jobs?q={}&l={}&radius={}&start={}&lang=en\".format(country_code, query, city, distance, page)\n",
    "    return url\n",
    "\n",
    "\n",
    "def create_url_linkedin(country, city, page, jobs_parameters):\n",
    "    \"\"\" Create url for linkedin scrapping\n",
    "    Args:\n",
    "        country: String, country name\n",
    "        city: String, city name\n",
    "        page: Integer, page numero\n",
    "        jobs_parameters: Dictionay, contains information about user request\n",
    "    Returns:\n",
    "        url: String, url\n",
    "    \"\"\"\n",
    "    geoId = find_geoId(city)    \n",
    "    query = jobs_parameters['query'].replace(' ', '%20')\n",
    "    distance = jobs_parameters['distance']\n",
    "    page = str(page*25)\n",
    "    url = \"https://www.linkedin.com/jobs/search/?geoId={}&keywords={}&location={}%20{}&start={}\".format(geoId, query, city, country, page)\n",
    "    return url\n",
    "\n",
    "\n",
    "def request_bs4(url, headers=None):\n",
    "    \"\"\" Make request with Beautiful Soup\n",
    "    Args:\n",
    "        url: String, url\n",
    "    Returns:\n",
    "        soup: Soup object, contains extracted data\n",
    "    \"\"\"\n",
    "    if headers is None:\n",
    "        # Use of headers to make HTTP requests\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'}\n",
    "    \n",
    "    # Extract data\n",
    "    request = requests.get(url, headers=headers)\n",
    "#     print(\"request status: {}\".format(request.status_code))\n",
    "    soup = BeautifulSoup(request.content, 'html.parser')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_data(website, country, city, page, jobs_parameters):\n",
    "    \"\"\" Extract data from website \n",
    "    Args:\n",
    "        website: String, website name\n",
    "        country: String, country name\n",
    "        city: String, city name\n",
    "        page: Integer, page numero\n",
    "        jobs_parameters: Dictionay, contains information about user request\n",
    "    Returns:\n",
    "        url: String, url\n",
    "        soup: Soup object, contains extracted data\n",
    "    \"\"\"\n",
    "    # Generate url\n",
    "    if website == 'Indeed':\n",
    "        url = create_url_indeed(country, city, page, jobs_parameters)\n",
    "    elif website == 'LinkedIn':\n",
    "        url = create_url_linkedin(country, city, page, jobs_parameters)\n",
    "    else:\n",
    "        print(f\"WEBSITE: '{website}'\")\n",
    "        \n",
    "    # Make request with Beautiful Soup\n",
    "    soup = request_bs4(url)\n",
    "    return url, soup\n",
    "       \n",
    "\n",
    "def transform_data(website, country, url, soup, jobs_parameters):\n",
    "    \"\"\" Create dictionary with job information\n",
    "    Args:\n",
    "        website: String, website name\n",
    "        country: String, country name\n",
    "        url: String, url\n",
    "        soup: Soup object, contains extracted data\n",
    "        jobs_parameters: Dictionay, contains information about user request\n",
    "    Returns:\n",
    "        job_info_tab: Array of strings, contains job information \n",
    "    \"\"\"\n",
    "    job_info_tab = []\n",
    "    \n",
    "    if website == 'Indeed':\n",
    "        whole_jobs = soup.find_all('div', class_=['mosaic-provider-jobcards'])\n",
    "        sample_jobs = whole_jobs[0].find_all('a', class_=['tapItem'])\n",
    "        \n",
    "    elif website == 'LinkedIn':\n",
    "        whole_jobs = soup.find_all(class_=\"base-card base-card--link base-search-card base-search-card--link job-search-card\")\n",
    "        sample_jobs = whole_jobs\n",
    "\n",
    "    # Retrieve title, company name, company location, salary, summary, date, id and url\n",
    "    for item in sample_jobs:\n",
    "        job_title = get_job_title(website, item, jobs_parameters)\n",
    "        if job_title != \"\":\n",
    "            job_company_name = get_job_company_name(website, item)\n",
    "            job_company_type = get_job_company_type(website, job_company_name)\n",
    "            job_company_sector = get_job_company_sector(website, job_company_name)\n",
    "            job_company_location = get_job_company_location(website, item)\n",
    "            job_salary = get_job_salary(website, item)\n",
    "            job_summary = get_job_summary(website, item)\n",
    "            job_date = get_job_date(website, item)\n",
    "            job_id = get_job_id(website, item)\n",
    "            job_url = get_job_url(website, item, url, job_id)\n",
    "            \n",
    "            country_code = get_country_code(country)\n",
    "            \n",
    "            # Create dictionary to retrieve data\n",
    "            job = {\n",
    "                'Title': job_title,\n",
    "                'Company': job_company_name,\n",
    "                'Company_type': job_company_type,\n",
    "                'Company_sector':job_company_sector,\n",
    "                'Country': country,\n",
    "                'Country_code': country_code,\n",
    "                'City': job_company_location,\n",
    "                'Salary': job_salary,\n",
    "                'Summary': job_summary,\n",
    "                'Date': job_date,\n",
    "                'Job_id': job_id,\n",
    "                'Job_url': job_url       \n",
    "            }\n",
    "            \n",
    "            # Add job dictionary into jobs tab\n",
    "            job_info_tab.append(job)\n",
    "            \n",
    "    return job_info_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403c48f-3bd3-492d-91b1-b067f6d9e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_title(title, title_keywords_ordered):\n",
    "    \"\"\" Rate job title\n",
    "    Args:\n",
    "        title: String, job title\n",
    "        title_keywords_ordered: Array of strings, contains title keywords ordered\n",
    "    Returns:\n",
    "        job_title_rating: Integer, job title rating\n",
    "    \"\"\"\n",
    "    job_title_rating = 0\n",
    "    for title_ordered in title_keywords_ordered:\n",
    "        if len(title_ordered) > 0:\n",
    "            if title_ordered.lower() in title.lower():\n",
    "                job_title_rating += 1\n",
    "    return job_title_rating\n",
    "\n",
    "\n",
    "def rate_company_size_type(company_size_type, company_size_type_ordered):\n",
    "    \"\"\" Rate job title\n",
    "    Args:\n",
    "        company_size_type: String, job company size type\n",
    "        company_size_type_ordered: Array of strings, contains company size type ordered\n",
    "    Returns:\n",
    "        job_company_size_type: Integer, job company size type rating\n",
    "    \"\"\"\n",
    "    job_company_size_type = 0\n",
    "    company_size_type_ordered = [company_size_type_key for company_size_type_key, company_size_type_value in company_size_type_ordered.items() if company_size_type_value is True]\n",
    "    for company_size in company_size_type_ordered:\n",
    "        if company_size_type == company_size:\n",
    "            job_company_size_type += 1\n",
    "    return job_company_size_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e862c-e8db-4267-85f2-57e574a94643",
   "metadata": {},
   "source": [
    "##### Function: main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237a382-4543-49df-93ed-55429c7a5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_jobs(df_jobs, jobs_parameters):\n",
    "    \"\"\" Rate jobs\n",
    "    Args:\n",
    "        df_jobs: Dataframe, contains information about scrapped jobs\n",
    "        jobs_parameters: Dictionay, contains information about user request\n",
    "    Returns:\n",
    "        df_jobs: Dataframe, contains information about scrapped jobs with general rating column\n",
    "    \"\"\"\n",
    "    df_rating = df_jobs['Job_id']\n",
    "    \n",
    "    # Rate title\n",
    "    title_keywords_ordered = [word.lower() for word in jobs_parameters['title_keywords_ordered']]\n",
    "    df_rating['Title rating'] = df_jobs['Title'].apply(rate_title, title_keywords_ordered=title_keywords_ordered)\n",
    "    \n",
    "    # Rate company size type\n",
    "    company_size_type_ordered = jobs_parameters['company_size_type']\n",
    "    df_rating['Company type rating'] = df_jobs['Company_type'].apply(rate_company_size_type, company_size_type_ordered=company_size_type_ordered)\n",
    "    \n",
    "    # Add general rating to job dataframe\n",
    "    df_rating['General rating'] =  df_rating['Title rating'] + df_rating['Company type rating']\n",
    "    df_jobs.insert(1, 'General rating', df_rating['General rating'])\n",
    "    return df_jobs\n",
    "    \n",
    "    \n",
    "def scrape_jobs(jobs_parameters):\n",
    "    \"\"\" Scrap jobs from several websites\n",
    "    Args:\n",
    "        jobs_parameters: Dictionay, contains information about user request\n",
    "    Returns:\n",
    "        df_jobs: Dataframe, contains information about scrapped jobs\n",
    "    \"\"\"\n",
    "    website_nb = 0\n",
    "    \n",
    "    # Loop on websites\n",
    "    for website in jobs_parameters['website']:\n",
    "        job_tab = []\n",
    "        countries_dic = create_countries_dic(jobs_parameters['location'])\n",
    "        \n",
    "        # Loop on countries\n",
    "        for country, cities in countries_dic.items():\n",
    "            \n",
    "            # Loop on cities\n",
    "            if type(cities) is str:\n",
    "                cities = [cities]\n",
    "            for city in cities:\n",
    "                \n",
    "                # Loop on pages\n",
    "                for page in range(0, jobs_parameters['pages']):\n",
    "                    \n",
    "                    # Extract whole data from 1 page\n",
    "                    url, soup = extract_data(website, country, city, page, jobs_parameters)\n",
    "                    print(url)\n",
    "#                     print(soup)\n",
    "\n",
    "                    # Create dictionary with job information\n",
    "                    job_dic = transform_data(website, country, url, soup, jobs_parameters)\n",
    "                    job_tab += job_dic\n",
    "\n",
    "                # Create df with jobs information\n",
    "                df = pd.DataFrame(data=job_tab, columns=['Title', 'Company', 'Company_type', 'Company_sector', 'Country', 'Country_code', 'City', 'Summary', 'Date', 'Job_id', 'Job_url'])\n",
    "                df.insert(0, 'Website', [website[0].upper() + website[1:] for i in range(len(df))])\n",
    "        \n",
    "        if website_nb == 0:\n",
    "            df_jobs = df\n",
    "        else:\n",
    "            df_jobs = pd.concat([df_jobs, df])\n",
    "        website_nb += 1\n",
    "        \n",
    "    df_jobs = df_jobs.drop_duplicates(subset=['Job_id'])\n",
    "    for col in list(df_jobs.columns):\n",
    "        df_jobs[col] = df_jobs[col].apply(remove_elements_end_sentence)\n",
    "        \n",
    "    # Rate jobs\n",
    "    df_jobs = rate_jobs(df_jobs, jobs_parameters)\n",
    "    df_jobs = df_jobs.sort_values(by='General rating', ascending=False).reset_index(drop=False)\n",
    "\n",
    "    return df_jobs\n",
    "\n",
    "\n",
    "def check_jobs_parameters(jobs_parameters):\n",
    "    \"\"\" Check jobs parameters values\n",
    "    Args:\n",
    "        jobs_parameters: Dictionary, contains jobs parameters\n",
    "    Returns:\n",
    "        website: String, websites\n",
    "        distance: Integer, distance from the location\n",
    "        pages: Integer, pages number\n",
    "    \"\"\"\n",
    "    website = jobs_parameters['website'] if len(jobs_parameters['website']) > 0 else \"Indeed\"\n",
    "    distance = int(jobs_parameters['distance']) if jobs_parameters['distance'].isdigit() else 0\n",
    "    pages = int(jobs_parameters['pages']) if jobs_parameters['pages'].isdigit() else 3\n",
    "    return website, distance, pages\n",
    "\n",
    "\n",
    "def read_jobs_parameters(json_jobs_parameters):\n",
    "    \"\"\" Read json jobs_parameters\n",
    "    Args:\n",
    "        json_jobs_parameters: String, json jobs parameters filename\n",
    "    Returns:\n",
    "        jobs_parameters: Dictionary, contains jobs parameters\n",
    "    \"\"\"\n",
    "    with open(json_jobs_parameters, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Check and set default value if no user request\n",
    "    website, distance, pages = check_jobs_parameters(data)\n",
    "\n",
    "    # Fill job parameters dictionary\n",
    "    jobs_parameters = {\n",
    "        'website': website,\n",
    "        'query': data['query'],\n",
    "        'location': data['location'],\n",
    "        'distance': distance,\n",
    "        'title_keywords_must': set(data['title_keywords_must']),\n",
    "        'title_keywords_excluded': set(data['title_keywords_excluded']),\n",
    "        'pages': pages,    \n",
    "        'title_keywords_ordered': set(data['title_keywords_ordered']),\n",
    "        'company_size_type': data['company_size_type'],\n",
    "    }\n",
    "    return jobs_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ac36e-d693-4889-992f-ff0f9a0931ac",
   "metadata": {},
   "source": [
    "##### Read jobs parameters from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c87562-9e30-45cf-b4e9-b7e0832839f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_jobs_parameters = \"../data/jobs_parameters_user_request.json\"\n",
    "jobs_parameters = read_jobs_parameters(json_jobs_parameters)\n",
    "print(\"Jobs parameters user request received:\\n{}\".format(jobs_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87513c3-d12c-4c47-a9f4-252c3d5e5425",
   "metadata": {},
   "source": [
    "##### Scrape jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9812b7b-47a4-4d61-9d9c-7a202a04b57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_jobs = scrape_jobs(jobs_parameters)\n",
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b7a2e-1781-4173-9196-3357f2a04052",
   "metadata": {},
   "source": [
    "##### Save jobs as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffafef-47df-4285-9a9b-c13e9a236d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_csv = \"../data/jobs2.csv\"\n",
    "save_df2csv(df_jobs, filename_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ff627-97f2-4232-b988-31c69739d993",
   "metadata": {},
   "source": [
    "##### Save jobs as json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966e8b9-46a3-456c-8126-628bb9a2c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = filename_csv\n",
    "json_filename = filename_csv.replace(\"csv\",\"json\")\n",
    "convert_csv2json(csv_filename, json_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_scrapping",
   "language": "python",
   "name": "kernel_scrapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
